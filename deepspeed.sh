# Sample commands to train
deepspeed train.py \
--wandb=True \
--seed=42 \
--root_dir='./' \
--expt='debugging' \
--reward_model_type=pool \
--pretrained_model=gpt2-large \
--tokenizer_type=gpt2 \
--max_length=600 \
--eval_fraction=0.1 \
--train_fraction=1.0 \
--output_dir=pm_checkpoint \
--num_train_epochs=1 \
--logging_steps=100 \
--gradient_accumulation_steps=1 \
--save_strategy=steps \
--save_total_limit=3 \
--save_steps=1000 \
--metric_for_best_model="eval_rank_accuracy" \
--greater_is_better=True \
--per_device_train_batch_size=8 \
--per_device_eval_batch_size=16 \
--eval_accumulation_steps=1 \
--evaluation_strategy=steps \
--eval_steps=1000 \
--logging_dir=logs \
--fp16=True \
--bf16=False \
--learning_rate=1e-5 \
--warmup_steps=1000 \
--torch_compile=False \
--auto_find_batch_size=True \
--deepspeed deepspeed_config.json \