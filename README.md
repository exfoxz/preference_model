Trains and compares a variety of preference models (reward models) with different losses and datasets.

### TODOs
- [ ] Add model.
- [ ] Add training code.
- [ ] Add evaluation code.
- [ ] Compare different losses.
- [ ] Compare different datasets.
- [ ] Add synthetic datasets.
- [ ] Add requirements.txt.
 
### Resources
- Code forked from https://github.com/CarperAI/trlx/tree/main/examples/summarize_rlhf.